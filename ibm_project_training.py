# -*- coding: utf-8 -*-
"""IBM PROJECT TRAINING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iqTJ1LwyRh6V-kkKgUxJfPF1NQggR1x0
"""

!pip install vaderSentiment # Install the vaderSentiment library

# Importing libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno

import warnings
warnings.filterwarnings('ignore')


plt.style.use('ggplot')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Instagram_data_by_Bhanu.csv', encoding='latin1')

df.head()

df.describe()

df.info()

# checking for null values
df.isna().sum()

# visualizing null values
msno.bar(df)

numeric_cols = df.select_dtypes(include=['number']).columns  # Select numeric columns only

plt.figure(figsize=(15, 10))
for plotnumber, column in enumerate(numeric_cols, 1):
    if plotnumber <= 30:
        ax = plt.subplot(5, 6, plotnumber)
        sns.histplot(df[column], kde=True)  # Use histplot instead of distplot (deprecated)
        plt.xlabel(column)

plt.tight_layout()
plt.show()

# heatmap

plt.figure(figsize = (20, 12))

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=['number'])

corr = numeric_df.corr()
mask = np.triu(np.ones_like(corr, dtype = bool))

sns.heatmap(corr, mask = mask, linewidths = 1, annot = True, fmt = ".2f")
plt.show()

print(df.columns)

import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import silhouette_score, mean_squared_error
import numpy as np
import joblib # Import joblib for saving the model

# Load the dataset
df_instagram = pd.read_csv("/content/drive/MyDrive/Instagram_data_by_Bhanu.csv", encoding="latin1")

# Step 1: Sentiment Analysis (VADER)
analyzer = SentimentIntensityAnalyzer()
df_instagram["sentiment_score"] = df_instagram["Caption"].astype(str).apply(lambda x: analyzer.polarity_scores(x)["compound"])
df_instagram["sentiment_label"] = df_instagram["sentiment_score"].apply(
    lambda x: "Positive" if x > 0.05 else ("Negative" if x < -0.05 else "Neutral")
)

# Step 2: Hashtag Clustering (TF-IDF + K-Means)
df_instagram["Hashtags"] = df_instagram["Hashtags"].fillna("NoHashtag")
df_instagram["Hashtags"] = df_instagram["Hashtags"].apply(lambda x: " ".join(x.split()))

df_filtered = df_instagram[df_instagram["Hashtags"] != "NoHashtag"]
if df_filtered["Hashtags"].nunique() > 1:
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(df_filtered["Hashtags"])

    num_clusters = 5
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    clusters = kmeans.fit_predict(X)

    silhouette_avg = silhouette_score(X, clusters)
    print(f"Silhouette Score for Hashtag Clustering: {silhouette_avg:.4f}")

    df_filtered["hashtag_cluster"] = clusters
else:
    print("❌ Not enough valid hashtags for clustering.")
    df_filtered["hashtag_cluster"] = -1

df_instagram = df_instagram.merge(df_filtered[["Hashtags", "hashtag_cluster"]], on="Hashtags", how="left")

# Step 3: Engagement Prediction (Random Forest Regression)
X = df_instagram[["Likes", "Shares", "Comments"]]
y = df_instagram["Impressions"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Engagement Prediction Model MSE: {mse:.4f}")

# Save the trained model using joblib
joblib.dump(rf, "engagement_model.pkl")

# Step 4: Hyperparameter Tuning
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, scoring="neg_mean_squared_error")
grid_search.fit(X_train, y_train)
print("Best Parameters for Engagement Prediction:", grid_search.best_params_)

# Step 5: Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
mse_scores = []
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    mse_scores.append(mean_squared_error(y_test, y_pred))

print(f"Average Cross-Validation MSE: {np.mean(mse_scores):.4f}")

# Save final dataset with results
df_instagram.to_csv("phase3_results.csv", index=False)
print("✅ Phase 3 Completed! Results saved as 'phase3_results.csv'")

from google.colab import files
files.download("phase3_results.csv")  # Replace with your actual file name

files.download("engagement_model.pkl")  # Replace with your actual model file